#!/usr/bin/env python3
"""
ìƒëª…ê³µí•™ìœ¡ì„±ì‹œí–‰ê³„íš ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ - ë©”ì¸ í”„ë¡œê·¸ë¨
PDF â†’ JSON â†’ ì •ê·œí™” â†’ Oracle DB ì ì¬ íŒŒì´í”„ë¼ì¸

ì‚¬ìš©ë²•:
    python main.py --batch            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê¶Œì¥)
    python main.py document.pdf       # íŠ¹ì • PDF íŒŒì¼ë§Œ ì²˜ë¦¬
    python main.py --skip-db          # DB ì ì¬ ê±´ë„ˆë›°ê¸°
    python main.py --workers 8        # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ì§€ì •
"""

import sys
import json
from pathlib import Path
import logging
from datetime import datetime
from typing import List
import argparse

# í•µì‹¬ ëª¨ë“ˆ
from extract_pdf_to_json import extract_pdf_to_json
from normalize_government_standard import GovernmentStandardNormalizer
from load_oracle_db import OracleDBLoader
from config import ORACLE_CONFIG

# ë°°ì¹˜ ì²˜ë¦¬
try:
    from batch_processor import BatchPDFProcessor, create_pdf_processor_func
    BATCH_AVAILABLE = True
except ImportError:
    BATCH_AVAILABLE = False
    BatchPDFProcessor = None
    create_pdf_processor_func = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PDFtoDBPipeline:
    
    def __init__(self,
                 skip_db: bool = False,
                 batch_mode: bool = False,
                 batch_size: int = 10,
                 max_workers: int = 4
                 ):
        """
        Args:
            skip_db: DB ì ì¬ ê±´ë„ˆë›°ê¸°
            batch_mode: ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
            batch_size: ë°°ì¹˜ë‹¹ íŒŒì¼ ìˆ˜
            max_workers: ë³‘ë ¬ ì‘ì—…ì ìˆ˜
        """
        self.skip_db = skip_db
        self.batch_mode = batch_mode
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.input_dir = Path("input")
        self.output_dir = Path("output")
        self.normalized_dir = Path("normalized_output_government")
        self.report_dir = Path("reports")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in [self.input_dir, self.output_dir, self.normalized_dir, self.report_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # í†µê³„
        self.stats = {
            'start_time': datetime.now(),
            'pdf_files': [],
            'processed': 0,
            'failed': 0,
            'total_records': 0,
            'db_loaded': False
        }
    
    def clean_previous_data(self):
        """ì´ì „ ì‹¤í–‰ ë°ì´í„° ì •ë¦¬ (JSON, CSV)"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ§¹ ì´ì „ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        logger.info("="*80)

        cleaned_items = []

        # 1. Output JSON íŒŒì¼ ì‚­ì œ
        json_files = list(self.output_dir.glob("*.json"))
        if json_files:
            for file in json_files:
                try:
                    file.unlink()
                    cleaned_items.append(f"JSON: {file.name}")
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file}: {e}")

        # 2. ì •ê·œí™”ëœ CSV íŒŒì¼ ì‚­ì œ
        csv_files = list(self.normalized_dir.glob("*.csv"))
        if csv_files:
            for file in csv_files:
                try:
                    file.unlink()
                    cleaned_items.append(f"CSV: {file.name}")
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file}: {e}")

        # ê²°ê³¼ ì¶œë ¥
        if cleaned_items:
            logger.info(f"âœ… ì´ {len(cleaned_items)}ê°œ í•­ëª© ì •ë¦¬ ì™„ë£Œ:")
            for item in cleaned_items[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                logger.info(f"   - {item}")
            if len(cleaned_items) > 10:
                logger.info(f"   ... ì™¸ {len(cleaned_items) - 10}ê°œ")
        else:
            logger.info("âœ… ì‚­ì œí•  ì´ì „ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

        logger.info("")

    def process_pdf(self, pdf_path: Path) -> bool:
        """ë‹¨ì¼ PDF ì²˜ë¦¬"""
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
            logger.info(f"{'='*60}")
            
            # 1. PDF â†’ JSON
            logger.info("1ï¸âƒ£ PDF â†’ JSON ë³€í™˜")
            json_data = extract_pdf_to_json(str(pdf_path), str(self.output_dir))
            
            if not json_data:
                logger.error("JSON ë³€í™˜ ì‹¤íŒ¨")
                return False
            
            # JSON íŒŒì¼ ì €ì¥
            json_file = self.output_dir / f"{pdf_path.stem}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"   âœ… JSON ìƒì„±: {json_file.name}")
            
            # 2. JSON â†’ ì •ê·œí™”
            logger.info("2ï¸âƒ£ ë°ì´í„° ì •ê·œí™”")
            normalizer = GovernmentStandardNormalizer(str(json_file), str(self.normalized_dir))
            
            if not normalizer.normalize(json_data):
                logger.error("ì •ê·œí™” ì‹¤íŒ¨")
                return False
            
            normalizer.save_to_csv()
            normalizer.print_statistics()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            for table_name, records in normalizer.data.items():
                if isinstance(records, list):
                    self.stats['total_records'] += len(records)
            
            logger.info(f"   âœ… ì •ê·œí™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def process_sample(self) -> bool:
        """ìƒ˜í”Œ ë°ì´í„° ì²˜ë¦¬"""
        try:
            logger.info("\n" + "="*60)
            logger.info("ğŸ§ª ìƒ˜í”Œ ë°ì´í„° ëª¨ë“œ")
            logger.info("="*60)
            
            # 1. ìƒ˜í”Œ JSON ìƒì„±
            logger.info("1ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ìƒì„±")
            json_data = extract_pdf_to_json(None, str(self.output_dir))
            
            # JSON ì €ì¥
            json_file = self.output_dir / "sample_data.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            # 2. ì •ê·œí™”
            logger.info("2ï¸âƒ£ ë°ì´í„° ì •ê·œí™”")
            normalizer = GovernmentStandardNormalizer(str(json_file), str(self.normalized_dir))
            normalizer.normalize(json_data)
            normalizer.save_to_csv()
            normalizer.print_statistics()
            
            # í†µê³„
            for table_name, records in normalizer.data.items():
                if isinstance(records, list):
                    self.stats['total_records'] += len(records)
            
            return True
            
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def process_batch_mode(self, pdf_files: List[str] = None) -> bool:
        """ëŒ€ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ"""
        if not BATCH_AVAILABLE:
            logger.error("ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. batch_processor.pyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return False
        
        try:
            logger.info("\n" + "="*60)
            logger.info("ğŸš€ ëŒ€ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ")
            logger.info("="*60)
            
            # 1. PDF â†’ JSON (ë°°ì¹˜ ì²˜ë¦¬)
            logger.info("1ï¸âƒ£ PDF â†’ JSON ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬)")
            
            processor = BatchPDFProcessor(
                input_dir=str(self.input_dir),
                output_dir=str(self.output_dir),
                batch_size=self.batch_size,
                max_workers=self.max_workers,
                use_multiprocessing=False  # ë©€í‹°ìŠ¤ë ˆë”© ì‚¬ìš© (ì•ˆì •ì„±)
            )
            
            pdf_processor_func = create_pdf_processor_func(str(self.output_dir))
            
            summary = processor.process_all(
                pdf_processor_func,
                recursive=False,
                save_results=True
            )
            
            processor.print_summary()
            
            if summary['processed'] == 0:
                logger.error("ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['processed'] = summary['processed']
            self.stats['failed'] = summary['failed']
            
            # 2. JSON â†’ ì •ê·œí™” (ëª¨ë“  íŒŒì¼ ëˆ„ì )
            logger.info("\n2ï¸âƒ£ ë°ì´í„° ì •ê·œí™”")
            
            json_files = list(self.output_dir.glob("*.json"))
            json_files = [f for f in json_files if not f.name.startswith('batch_')]
            
            logger.info(f"ì •ê·œí™”í•  JSON íŒŒì¼: {len(json_files)}ê°œ")
            
            # ì „ì²´ JSON ë°ì´í„° ìˆ˜ì§‘
            all_json_data = []
            for i, json_file in enumerate(json_files, 1):
                if i % 50 == 0:
                    logger.info(f"  JSON ë¡œë“œ ì¤‘: [{i}/{len(json_files)}]")

                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                        all_json_data.append(json_data)
                except Exception as e:
                    logger.error(f"JSON ë¡œë“œ ì‹¤íŒ¨ {json_file.name}: {e}")

            logger.info(f"âœ… {len(all_json_data)}ê°œ JSON ë¡œë“œ ì™„ë£Œ")

            # ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì •ê·œí™” (íŒŒì¼ë³„ë¡œ ì—°ë„ ì¶”ì¶œ)
            if all_json_data:
                logger.info("ëª¨ë“  ë°ì´í„° í†µí•© ì •ê·œí™” ì‹œì‘...")

                # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ normalizer ì´ˆê¸°í™”
                normalizer = GovernmentStandardNormalizer(
                    str(json_files[0]),
                    str(self.normalized_dir)
                )

                # ê° JSON íŒŒì¼ë³„ë¡œ ì²˜ë¦¬ (ì—°ë„ ì¶”ì¶œ í¬í•¨)
                for json_file, json_data in zip(json_files, all_json_data):
                    # íŒŒì¼ë§ˆë‹¤ ì—°ë„ë¥¼ ì¶”ì¶œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                    import re
                    filename = json_file.stem
                    year_match = re.search(r'(20\d{2})', filename)

                    if year_match:
                        doc_year = int(year_match.group(1))
                        logger.info(f"ğŸ“… {filename} -> {doc_year}ë…„ë„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

                        # ì—°ë„ë³„ë¡œ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                        normalizer.current_context['document_year'] = doc_year
                        normalizer.current_context['performance_year'] = doc_year - 1
                        normalizer.current_context['plan_year'] = doc_year

                    normalizer.normalize(json_data)

                # í•œ ë²ˆì— CSV ì €ì¥
                normalizer.save_to_csv()
                normalizer.print_statistics()

                # í†µê³„
                for table_name, records in normalizer.data.items():
                    if isinstance(records, list):
                        self.stats['total_records'] += len(records)

                logger.info(f"âœ… ì •ê·œí™” ì™„ë£Œ: ì´ {self.stats['total_records']:,}ê±´")
            else:
                logger.error("ì •ê·œí™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False

            return True
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def load_to_database(self) -> bool:
        """3ë‹¨ê³„: Oracle ë°ì´í„°ë² ì´ìŠ¤ ì ì¬"""
        if self.skip_db:
            logger.info("\nâ­ï¸ DB ì ì¬ ê±´ë„ˆëœ€")
            return True

        try:
            logger.info("\n" + "="*60)
            logger.info("3ï¸âƒ£ Oracle ë°ì´í„°ë² ì´ìŠ¤ ì ì¬")
            logger.info("="*60)

            # Oracle ì ì¬
            oracle_loader = OracleDBLoader(ORACLE_CONFIG, str(self.normalized_dir))
            oracle_loader.connect()

            # í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°)
            oracle_loader.create_tables()

            # ë°ì´í„° ì ì¬
            oracle_loader.load_all_tables()

            oracle_loader.close()

            self.stats['db_loaded'] = True
            logger.info(f"   âœ… Oracle DB ì ì¬ ì™„ë£Œ: {oracle_loader.load_stats['total_records']:,}ê±´")

            return True

        except Exception as e:
            logger.error(f"Oracle DB ì ì¬ ì‹¤íŒ¨: {e}")
            logger.warning("âš ï¸ Oracle ì ì¬ ì‹¤íŒ¨í–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            return False

    def generate_report(self):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("="*80)
        report.append("ğŸ“Š PDF to Database ì²˜ë¦¬ ë³´ê³ ì„œ")
        report.append("="*80)
        report.append(f"ì‹¤í–‰ ì‹œê°„: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ì†Œìš” ì‹œê°„: {(datetime.now() - self.stats['start_time']).total_seconds():.1f}ì´ˆ")
        report.append("")
        
        if self.stats['pdf_files']:
            report.append("ğŸ“„ ì²˜ë¦¬ëœ íŒŒì¼:")
            for pdf in self.stats['pdf_files']:
                report.append(f"  - {pdf}")
        
        report.append("")
        report.append("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        report.append(f"  â€¢ ì„±ê³µ: {self.stats['processed']}ê°œ")
        report.append(f"  â€¢ ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
        report.append(f"  â€¢ ì´ ë ˆì½”ë“œ: {self.stats['total_records']:,}ê±´")
        report.append(f"  â€¢ DB ì ì¬: {'âœ…' if self.stats['db_loaded'] else 'â­ï¸ ê±´ë„ˆëœ€'}")
        report.append("")
        
        # ìƒì„±ëœ íŒŒì¼
        report.append("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
        report.append(f"  â€¢ JSON: {self.output_dir}/*.json")
        report.append(f"  â€¢ CSV: {self.normalized_dir}/*.csv")
        if self.stats['db_loaded']:
            report.append(f"  â€¢ DB: government_standard database")
        
        report.append("")
        report.append("="*80)
        
        # ë³´ê³ ì„œ ì €ì¥
        report_text = "\n".join(report)
        report_file = self.report_dir / f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        # ì½˜ì†” ì¶œë ¥
        print("\n" + report_text)
        
        return report_file
    
    def run(self, pdf_files: List[str] = None):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ PDF to Database íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("="*80)
        
        # ì´ì „ ë°ì´í„° ì •ë¦¬
        self.clean_previous_data()

        success = False
        
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
            if self.batch_mode:
                success = self.process_batch_mode(pdf_files)
            
            # ì¼ë°˜ PDF ì²˜ë¦¬ ëª¨ë“œ
            else:
                # PDF íŒŒì¼ ì°¾ê¸°
                if pdf_files:
                    pdf_list = [Path(f) for f in pdf_files if Path(f).exists()]
                else:
                    # input í´ë”ì—ì„œ ëª¨ë“  PDF ì°¾ê¸°
                    pdf_list = list(self.input_dir.glob("*.pdf"))
                
                if not pdf_list:
                    logger.warning("PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„° ëª¨ë“œë¡œ ì „í™˜...")
                    success = self.process_sample()
                    self.stats['processed'] = 1 if success else 0
                else:
                    # ê° PDF ì²˜ë¦¬
                    for pdf_path in pdf_list:
                        self.stats['pdf_files'].append(pdf_path.name)
                        
                        if self.process_pdf(pdf_path):
                            self.stats['processed'] += 1
                        else:
                            self.stats['failed'] += 1
                    
                    success = self.stats['processed'] > 0
            
            # DB ì ì¬
            if success and not self.skip_db:
                self.load_to_database()

            # ë³´ê³ ì„œ ìƒì„±
            report_file = self.generate_report()
            logger.info(f"\nğŸ“„ ë³´ê³ ì„œ ìƒì„±: {report_file}")
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            success = False
        
        # ì™„ë£Œ ë©”ì‹œì§€
        if success:
            print("\n" + "="*80)
            print("âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            print("="*80)
        else:
            print("\n" + "="*80)
            print("âš ï¸ íŒŒì´í”„ë¼ì¸ ì¼ë¶€ ì‹¤íŒ¨")
            print("="*80)
        
        return success


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ìƒëª…ê³µí•™ìœ¡ì„±ì‹œí–‰ê³„íš PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  python main.py --batch            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê¶Œì¥)
  python main.py doc1.pdf           # íŠ¹ì • PDF íŒŒì¼ ì²˜ë¦¬
  python main.py --skip-db          # DB ì ì¬ ê±´ë„ˆë›°ê¸°
  python main.py --workers 8        # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ì§€ì •
        """
    )
    
    parser.add_argument(
        'pdf_files',
        nargs='*',
        help='ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ (ìƒëµí•˜ë©´ input í´ë” ê²€ìƒ‰)'
    )
    
    parser.add_argument(
        '--skip-db',
        action='store_true',
        help='ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ê±´ë„ˆë›°ê¸°'
    )
    
    parser.add_argument(
        '--batch',
        action='store_true',
        help='ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (ë³‘ë ¬ ì²˜ë¦¬)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='ë°°ì¹˜ë‹¹ íŒŒì¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)'
    )
    
    parser.add_argument(
        '--workers',
        type=int,
        default=4,
        help='ë³‘ë ¬ ì‘ì—…ì ìˆ˜ (ê¸°ë³¸ê°’: 4)'
    )
    
    args = parser.parse_args()
    
    # ë°°ì¹˜ ëª¨ë“œ ì²´í¬
    if args.batch and not BATCH_AVAILABLE:
        print("âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install tqdm")
        return 1
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = PDFtoDBPipeline(
        skip_db=args.skip_db,
        batch_mode=args.batch,
        batch_size=args.batch_size,
        max_workers=args.workers
    )
    
    success = pipeline.run(args.pdf_files)
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())