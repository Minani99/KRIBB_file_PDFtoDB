#!/usr/bin/env python3
"""
ìƒëª…ê³µí•™ìœ¡ì„±ì‹œí–‰ê³„íš ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ - ë©”ì¸ í”„ë¡œê·¸ë¨
PDF â†’ JSON â†’ ì •ê·œí™” â†’ Oracle DB ì ì¬ íŒŒì´í”„ë¼ì¸

ì‚¬ìš©ë²•:
    python main.py --batch            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê¶Œì¥)
    python main.py document.pdf       # íŠ¹ì • PDF íŒŒì¼ë§Œ ì²˜ë¦¬
    python main.py --skip-db          # DB ì ì¬ ê±´ë„ˆë›°ê¸°
    python main.py --workers 8        # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ì§€ì •
"""

import sys
import io
import json
from pathlib import Path
import logging
from datetime import datetime
from typing import List
import argparse

# UTF-8 ì¶œë ¥ ì„¤ì • (Windows cp949 ì—ëŸ¬ ë°©ì§€)
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
if sys.stderr.encoding != 'utf-8':
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# í•µì‹¬ ëª¨ë“ˆ
from extract_pdf_to_json import extract_pdf_to_json
from normalize_government_standard import GovernmentStandardNormalizer
from load_oracle_direct import OracleDirectLoader
from config import (
    ORACLE_CONFIG,
    ORACLE_CONFIG_DEV,
    INPUT_DIR,
    OUTPUT_DIR,
    NORMALIZED_OUTPUT_GOVERNMENT_DIR
)

# ë°°ì¹˜ ì²˜ë¦¬
try:
    from batch_processor import BatchPDFProcessor, create_pdf_processor_func
    BATCH_AVAILABLE = True
except ImportError:
    BATCH_AVAILABLE = False
    BatchPDFProcessor = None
    create_pdf_processor_func = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PDFtoDBPipeline:
    
    def __init__(self,
                 skip_db: bool = False,
                 batch_mode: bool = False,
                 batch_size: int = 10,
                 max_workers: int = 5
                 ):
        """
        Args:
            skip_db: DB ì ì¬ ê±´ë„ˆë›°ê¸°
            batch_mode: ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
            batch_size: ë°°ì¹˜ë‹¹ íŒŒì¼ ìˆ˜
            max_workers: ë³‘ë ¬ ì‘ì—…ì ìˆ˜
        """
        self.skip_db = skip_db
        self.batch_mode = batch_mode
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        # ë””ë ‰í† ë¦¬ ì„¤ì • (configì—ì„œ ê°€ì ¸ì˜´)
        self.input_dir = INPUT_DIR
        self.output_dir = OUTPUT_DIR
        self.normalized_dir = NORMALIZED_OUTPUT_GOVERNMENT_DIR

        # í†µê³„
        self.stats = {
            'start_time': datetime.now(),
            'pdf_files': [],
            'processed': 0,
            'failed': 0,
            'total_records': 0,
            'db_loaded': False,
            'matched': 0,
            'unmatched': 0,
            'diff_found': 0
        }
    
    def clean_previous_data(self):
        """ì´ì „ ì‹¤í–‰ ë°ì´í„° ì •ë¦¬ (JSON, CSV)"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ§¹ ì´ì „ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        logger.info("="*80)

        cleaned_items = []

        # 1. Output JSON íŒŒì¼ ì‚­ì œ
        json_files = list(self.output_dir.glob("*.json"))
        if json_files:
            for file in json_files:
                try:
                    file.unlink()
                    cleaned_items.append(f"JSON: {file.name}")
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file}: {e}")

        # 2. ì •ê·œí™”ëœ CSV íŒŒì¼ ì‚­ì œ
        csv_files = list(self.normalized_dir.glob("*.csv"))
        if csv_files:
            for file in csv_files:
                try:
                    file.unlink()
                    cleaned_items.append(f"CSV: {file.name}")
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file}: {e}")

        # ê²°ê³¼ ì¶œë ¥
        if cleaned_items:
            logger.info(f"âœ… ì´ {len(cleaned_items)}ê°œ í•­ëª© ì •ë¦¬ ì™„ë£Œ:")
            for item in cleaned_items[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                logger.info(f"   - {item}")
            if len(cleaned_items) > 10:
                logger.info(f"   ... ì™¸ {len(cleaned_items) - 10}ê°œ")
        else:
            logger.info("âœ… ì‚­ì œí•  ì´ì „ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

        logger.info("")

    def process_pdf(self, pdf_path: Path) -> bool:
        """ë‹¨ì¼ PDF ì²˜ë¦¬"""
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
            logger.info(f"{'='*60}")
            
            # 1. PDF â†’ JSON
            logger.info("1ï¸âƒ£ PDF â†’ JSON ë³€í™˜")
            json_data = extract_pdf_to_json(str(pdf_path), str(self.output_dir))
            
            if not json_data:
                logger.error("JSON ë³€í™˜ ì‹¤íŒ¨")
                return False
            
            # JSON íŒŒì¼ ì €ì¥
            json_file = self.output_dir / f"{pdf_path.stem}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"   âœ… JSON ìƒì„±: {json_file.name}")
            
            # 2. JSON â†’ ì •ê·œí™” (DB ì—°ê²° ì „ë‹¬í•˜ì—¬ PLAN_ID ë§¤ì¹­)
            logger.info("2ï¸âƒ£ ë°ì´í„° ì •ê·œí™”")

            # Oracle DB ì—°ê²° (PLAN_ID ë§¤ì¹­ìš©)
            from oracle_db_manager import OracleDBManager
            db_manager = None
            if not self.skip_db:
                try:
                    db_manager = OracleDBManager(ORACLE_CONFIG)
                    db_manager.connect()
                    logger.info("   ğŸ”— DB ì—°ê²° (PLAN_ID ë§¤ì¹­ìš©)")
                except Exception as e:
                    logger.warning(f"   âš ï¸ DB ì—°ê²° ì‹¤íŒ¨ (ì‹ ê·œ PLAN_IDë¡œ ìƒì„±): {e}")
                    db_manager = None

            normalizer = GovernmentStandardNormalizer(
                str(json_file),
                str(self.normalized_dir),
                db_manager=db_manager
            )

            if not normalizer.normalize(json_data):
                logger.error("ì •ê·œí™” ì‹¤íŒ¨")
                if db_manager:
                    db_manager.close()
                return False
            
            normalizer.save_to_csv()
            normalizer.print_statistics()
            
            # DB ì—°ê²° ì¢…ë£Œ
            if db_manager:
                db_manager.close()

            # í†µê³„ ì—…ë°ì´íŠ¸
            for table_name, records in normalizer.data.items():
                if isinstance(records, list):
                    self.stats['total_records'] += len(records)
            
            logger.info(f"   âœ… ì •ê·œí™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def process_batch_mode(self, pdf_files: List[str] = None) -> bool:
        """ëŒ€ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ"""
        if not BATCH_AVAILABLE:
            logger.error("ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. batch_processor.pyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return False
        
        try:
            logger.info("\n" + "="*60)
            logger.info("ğŸš€ ëŒ€ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ")
            logger.info("="*60)
            
            # 1. PDF â†’ JSON (ë°°ì¹˜ ì²˜ë¦¬)
            logger.info("1ï¸âƒ£ PDF â†’ JSON ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬)")
            
            processor = BatchPDFProcessor(
                input_dir=str(self.input_dir),
                output_dir=str(self.output_dir),
                batch_size=self.batch_size,
                max_workers=self.max_workers,
                use_multiprocessing=False  # ë©€í‹°ìŠ¤ë ˆë”© ì‚¬ìš©
            )
            
            pdf_processor_func = create_pdf_processor_func(str(self.output_dir))
            
            summary = processor.process_all(
                pdf_processor_func,
                recursive=False,
                save_results=True
            )
            
            processor.print_summary()
            
            if summary['processed'] == 0:
                logger.error("ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['processed'] = summary['processed']
            self.stats['failed'] = summary['failed']
            
            # 2. JSON â†’ ì •ê·œí™” (ëª¨ë“  íŒŒì¼ ëˆ„ì )
            logger.info("\n2ï¸âƒ£ ë°ì´í„° ì •ê·œí™”")
            
            json_files = list(self.output_dir.glob("*.json"))
            json_files = [f for f in json_files if not f.name.startswith('batch_')]
            
            logger.info(f"ì •ê·œí™”í•  JSON íŒŒì¼: {len(json_files)}ê°œ")
            
            # ì „ì²´ JSON ë°ì´í„° ìˆ˜ì§‘
            all_json_data = []
            for i, json_file in enumerate(json_files, 1):
                if i % 50 == 0:
                    logger.info(f"  JSON ë¡œë“œ ì¤‘: [{i}/{len(json_files)}]")

                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                        all_json_data.append(json_data)
                except Exception as e:
                    logger.error(f"JSON ë¡œë“œ ì‹¤íŒ¨ {json_file.name}: {e}")

            logger.info(f"âœ… {len(all_json_data)}ê°œ JSON ë¡œë“œ ì™„ë£Œ")

            # ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì •ê·œí™” (íŒŒì¼ë³„ë¡œ ì—°ë„ ì¶”ì¶œ)
            if all_json_data:
                logger.info("ëª¨ë“  ë°ì´í„° í†µí•© ì •ê·œí™” ì‹œì‘...")

                # Oracle DB ì—°ê²° (PLAN_ID ë§¤ì¹­ìš©)
                from oracle_db_manager import OracleDBManager
                db_manager = None
                if not self.skip_db:
                    try:
                        db_manager = OracleDBManager(ORACLE_CONFIG)
                        db_manager.connect()
                        logger.info("   ğŸ”— DB ì—°ê²° (PLAN_ID ë§¤ì¹­ìš©)")
                    except Exception as e:
                        logger.warning(f"   âš ï¸ DB ì—°ê²° ì‹¤íŒ¨ (ì‹ ê·œ PLAN_IDë¡œ ìƒì„±): {e}")
                        db_manager = None

                # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ normalizer ì´ˆê¸°í™”
                normalizer = GovernmentStandardNormalizer(
                    str(json_files[0]),
                    str(self.normalized_dir),
                    db_manager=db_manager
                )

                # ê° JSON íŒŒì¼ë³„ë¡œ ì²˜ë¦¬ (ì—°ë„ ì¶”ì¶œ í¬í•¨)
                for json_file, json_data in zip(json_files, all_json_data):
                    # íŒŒì¼ë§ˆë‹¤ ì—°ë„ë¥¼ ì¶”ì¶œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                    import re
                    filename = json_file.stem
                    year_match = re.search(r'(20\d{2})', filename)

                    if year_match:
                        doc_year = int(year_match.group(1))
                        logger.info(f"ğŸ“… {filename} -> {doc_year}ë…„ë„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

                        # ì—°ë„ë³„ë¡œ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                        normalizer.current_context['document_year'] = doc_year
                        normalizer.current_context['performance_year'] = doc_year - 1
                        normalizer.current_context['plan_year'] = doc_year

                    normalizer.normalize(json_data)

                # í•œ ë²ˆì— CSV ì €ì¥
                normalizer.save_to_csv()
                normalizer.print_statistics()

                # DB ì—°ê²° ì¢…ë£Œ
                if db_manager:
                    db_manager.close()
                    logger.info("   ğŸ”Œ DB ì—°ê²° ì¢…ë£Œ")

                # í†µê³„
                for table_name, records in normalizer.data.items():
                    if isinstance(records, list):
                        self.stats['total_records'] += len(records)

                logger.info(f"âœ… ì •ê·œí™” ì™„ë£Œ: ì´ {self.stats['total_records']:,}ê±´")
            else:
                logger.error("ì •ê·œí™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False

            return True
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def load_to_database(self) -> bool:
        """3ë‹¨ê³„: Oracle ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ (ë§¤ì¹­ ê¸°ë°˜)"""
        if self.skip_db:
            logger.info("\nâ­ï¸ DB ì ì¬ ê±´ë„ˆëœ€")
            return True

        try:
            logger.info("\n" + "="*80)
            logger.info("3ï¸âƒ£ Oracle ë°ì´í„°ë² ì´ìŠ¤ ì ì¬")
            logger.info("="*80)

            # Oracle ì ì¬ (ë§¤ì¹­ ê¸°ë°˜)
            # ORACLE_CONFIG: TB_PLAN_DATA ì½ê¸° (ë§¤ì¹­ìš©)
            # ORACLE_CONFIG_DEV: í•˜ìœ„ í…Œì´ë¸” ì“°ê¸° (ì ì¬ìš©)
            oracle_loader = OracleDirectLoader(
                db_config_read=ORACLE_CONFIG,
                db_config_write=ORACLE_CONFIG_DEV,
                csv_dir=str(self.normalized_dir)
            )
            oracle_loader.connect()

            logger.info("\nğŸ“‹ íŒŒì´í”„ë¼ì¸ íë¦„:")
            logger.info("   1ï¸âƒ£ BICS.TB_PLAN_DATA ì¡°íšŒ (ê¸°ì¡´ ë ˆì½”ë“œ - ë§¤ì¹­ìš©)")
            logger.info("   2ï¸âƒ£ CSVì™€ ë§¤ì¹­ (YEAR + BIZ_NM + DETAIL_BIZ_NM ê¸°ì¤€)")
            logger.info("   3ï¸âƒ£ ë§¤ì¹­ ì„±ê³µ â†’ ê¸°ì¡´ PLAN_ID ì¬ì‚¬ìš©")
            logger.info("   4ï¸âƒ£ ë§¤ì¹­ ì‹¤íŒ¨ â†’ ì‹ ê·œ ë ˆì½”ë“œë¡œ í‘œì‹œ")
            logger.info("   5ï¸âƒ£ í•˜ìœ„ í…Œì´ë¸” ì ì¬ â†’ BICS_DEV ìŠ¤í‚¤ë§ˆ (TB_PLAN_BUDGET, SCHEDULE, PERFORMANCE, ACHIEVEMENTS)")

            # ë§¤ì¹­ ê¸°ë°˜ ì ì¬ ì‹¤í–‰
            oracle_loader.load_with_matching()

            # í†µê³„ ì¶œë ¥
            stats = oracle_loader.load_stats

            logger.info("\n" + "="*80)
            logger.info("ğŸ“Š ì ì¬ ì™„ë£Œ í†µê³„")
            logger.info("="*80)
            logger.info(f"âœ… ì´ ì ì¬ ë ˆì½”ë“œ: {stats['total_records']:,}ê±´")
            logger.info(f"\nğŸ“Œ ë§¤ì¹­ ê²°ê³¼:")
            logger.info(f"   â€¢ ë§¤ì¹­ ì„±ê³µ: {stats['matched']}ê±´ (ê¸°ì¡´ PLAN_ID ì¬ì‚¬ìš©)")
            logger.info(f"   â€¢ ë§¤ì¹­ ì‹¤íŒ¨: {stats['unmatched']}ê±´ (ì‹ ê·œ ë ˆì½”ë“œ)")
            logger.info(f"   â€¢ ì°¨ì´ì  ë°œê²¬: {stats['diff_found']}ê±´ (ë‚´ìš© ë¶ˆì¼ì¹˜)")

            if stats['unmatched'] > 0:
                logger.warning(f"\nâš ï¸  ë§¤ì¹­ ì‹¤íŒ¨ {stats['unmatched']}ê±´ì€ ì‹ ê·œ ë‚´ì—­ì‚¬ì—…ìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.")
                logger.warning("   â†’ ë§¤ì¹­ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì—¬ ìˆ˜ë™ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            if stats['diff_found'] > 0:
                logger.warning(f"\nâš ï¸  ì°¨ì´ì  ë°œê²¬ {stats['diff_found']}ê±´ì€ ë‚´ìš©ì´ ë³€ê²½ëœ ì‚¬ì—…ì…ë‹ˆë‹¤.")
                logger.warning("   â†’ ì—…ë°ì´íŠ¸ ì—¬ë¶€ë¥¼ ê²€í† í•´ì£¼ì„¸ìš”.")

            oracle_loader.close()

            self.stats['db_loaded'] = True
            self.stats['matched'] = stats['matched']
            self.stats['unmatched'] = stats['unmatched']
            self.stats['diff_found'] = stats['diff_found']

            return True

        except Exception as e:
            logger.error(f"âŒ Oracle DB ì ì¬ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            logger.warning("âš ï¸ Oracle ì ì¬ ì‹¤íŒ¨í–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            return False

    def run(self, pdf_files: List[str] = None):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ PDF to Database íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("="*80)
        
        # ì´ì „ ë°ì´í„° ì •ë¦¬
        self.clean_previous_data()

        success = False
        
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
            if self.batch_mode:
                success = self.process_batch_mode(pdf_files)
            
            # ì¼ë°˜ PDF ì²˜ë¦¬ ëª¨ë“œ
            else:
                # PDF íŒŒì¼ ì°¾ê¸°
                if pdf_files:
                    pdf_list = [Path(f) for f in pdf_files if Path(f).exists()]
                else:
                    # input í´ë”ì—ì„œ ëª¨ë“  PDF ì°¾ê¸°
                    pdf_list = list(self.input_dir.glob("*.pdf"))
                
                if not pdf_list:
                    logger.error("âŒ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                    logger.error(f"   '{self.input_dir}' í´ë”ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
                    return False

                # ê° PDF ì²˜ë¦¬
                for pdf_path in pdf_list:
                    self.stats['pdf_files'].append(pdf_path.name)

                    if self.process_pdf(pdf_path):
                        self.stats['processed'] += 1
                    else:
                        self.stats['failed'] += 1

                success = self.stats['processed'] > 0

            # DB ì ì¬
            if success and not self.skip_db:
                self.load_to_database()

            # ìµœì¢… í†µê³„ ì¶œë ¥
            logger.info("\n" + "="*80)
            logger.info("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
            logger.info("="*80)
            logger.info(f"ì„±ê³µ: {self.stats['processed']}ê°œ")
            logger.info(f"ì‹¤íŒ¨: {self.stats['failed']}ê°œ")
            logger.info(f"ì´ ë ˆì½”ë“œ: {self.stats['total_records']:,}ê±´")
            logger.info(f"DB ì ì¬: {'âœ…' if self.stats['db_loaded'] else 'â­ï¸ ê±´ë„ˆëœ€'}")

            if self.stats['db_loaded']:
                logger.info(f"\nğŸ“Œ ë§¤ì¹­ ê²°ê³¼:")
                logger.info(f"   â€¢ ë§¤ì¹­ ì„±ê³µ: {self.stats.get('matched', 0)}ê±´")
                logger.info(f"   â€¢ ë§¤ì¹­ ì‹¤íŒ¨: {self.stats.get('unmatched', 0)}ê±´")
                logger.info(f"   â€¢ ì°¨ì´ì  ë°œê²¬: {self.stats.get('diff_found', 0)}ê±´")

        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            success = False
        
        # ì™„ë£Œ ë©”ì‹œì§€
        if success:
            print("\n" + "="*80)
            print("âœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            print("="*80)
        else:
            print("\n" + "="*80)
            print("âš ï¸ íŒŒì´í”„ë¼ì¸ ì¼ë¶€ ì‹¤íŒ¨")
            print("="*80)
        
        return success


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  python main.py --batch            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
  python main.py doc1.pdf           # íŠ¹ì • PDF íŒŒì¼ ì²˜ë¦¬
  python main.py --skip-db          # DB ì ì¬ ê±´ë„ˆë›°ê¸°
  python main.py --workers 8        # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ì§€ì •
        """
    )
    
    parser.add_argument(
        'pdf_files',
        nargs='*',
        help='ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ (ìƒëµí•˜ë©´ input í´ë” ê²€ìƒ‰)'
    )
    
    parser.add_argument(
        '--skip-db',
        action='store_true',
        help='ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ê±´ë„ˆë›°ê¸°'
    )
    
    parser.add_argument(
        '--batch',
        action='store_true',
        help='ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ (ë³‘ë ¬ ì²˜ë¦¬)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='ë°°ì¹˜ë‹¹ íŒŒì¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)'
    )
    
    parser.add_argument(
        '--workers',
        type=int,
        default=5,
        help='ë³‘ë ¬ ì‘ì—…ì ìˆ˜ (ê¸°ë³¸ê°’: 5)'
    )
    
    args = parser.parse_args()
    
    # ë°°ì¹˜ ëª¨ë“œ ì²´í¬
    if args.batch and not BATCH_AVAILABLE:
        print("âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install tqdm")
        return 1
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = PDFtoDBPipeline(
        skip_db=args.skip_db,
        batch_mode=args.batch,
        batch_size=args.batch_size,
        max_workers=args.workers
    )
    
    success = pipeline.run(args.pdf_files)
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())