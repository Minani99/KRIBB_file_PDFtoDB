#!/usr/bin/env python3
"""
ìƒëª…ê³µí•™ìœ¡ì„±ì‹œí–‰ê³„íš ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ - ë©”ì¸ í”„ë¡œê·¸ë¨
PDF â†’ JSON â†’ ì •ê·œí™” â†’ Oracle DB ì ì¬ íŒŒì´í”„ë¼ì¸

ì‚¬ìš©ë²•:
    python main.py                    # input í´ë”ì˜ ëª¨ë“  PDF ì²˜ë¦¬
    python main.py document.pdf       # íŠ¹ì • PDF íŒŒì¼ë§Œ ì²˜ë¦¬
    python main.py --skip-db          # DB ì ì¬ ê±´ë„ˆë›°ê¸° (CSVë§Œ ìƒì„±)
"""

import sys
import io
import json
import re
from pathlib import Path
import logging
from datetime import datetime
from typing import List
import argparse

# UTF-8 ì¶œë ¥ ì„¤ì • (Windows cp949 ì—ëŸ¬ ë°©ì§€)
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
if sys.stderr.encoding != 'utf-8':
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# í•µì‹¬ ëª¨ë“ˆ
from extract_pdf_to_json import extract_pdf_to_json
from normalize_government_standard import GovernmentStandardNormalizer
from config import (
    INPUT_DIR,
    OUTPUT_DIR,
    NORMALIZED_OUTPUT_GOVERNMENT_DIR
)

# DB ëª¨ë“ˆ (ì„ íƒì )
try:
    from load_oracle_direct import OracleDirectLoader
    from oracle_db_manager import OracleDBManager
    from config import ORACLE_CONFIG, ORACLE_CONFIG_DEV
    DB_AVAILABLE = True
except ImportError as e:
    DB_AVAILABLE = False
    print(f"âš ï¸ DB ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨ (CSVë§Œ ìƒì„±): {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PDFtoDBPipeline:
    """PDF â†’ DB íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, skip_db: bool = False):
        """
        Args:
            skip_db: DB ì ì¬ ê±´ë„ˆë›°ê¸°
        """
        self.skip_db = skip_db or not DB_AVAILABLE
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.input_dir = Path(INPUT_DIR)
        self.output_dir = Path(OUTPUT_DIR)
        self.normalized_dir = Path(NORMALIZED_OUTPUT_GOVERNMENT_DIR)

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.input_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        self.normalized_dir.mkdir(exist_ok=True)

        # í†µê³„
        self.stats = {
            'start_time': datetime.now(),
            'pdf_files': [],
            'processed': 0,
            'failed': 0,
            'total_records': 0,
            'db_loaded': False,
            'matched': 0,
            'unmatched': 0
        }
    
    def clean_previous_data(self):
        """ì´ì „ ì‹¤í–‰ ë°ì´í„° ì •ë¦¬"""
        logger.info("=" * 80)
        logger.info("ğŸ§¹ ì´ì „ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        logger.info("=" * 80)

        cleaned = 0

        # JSON íŒŒì¼ ì‚­ì œ
        for file in self.output_dir.glob("*.json"):
            try:
                file.unlink()
                cleaned += 1
            except Exception as e:
                logger.warning(f"ì‚­ì œ ì‹¤íŒ¨ {file}: {e}")

        # CSV íŒŒì¼ ì‚­ì œ
        for file in self.normalized_dir.glob("*.csv"):
            try:
                file.unlink()
                cleaned += 1
            except Exception as e:
                logger.warning(f"ì‚­ì œ ì‹¤íŒ¨ {file}: {e}")

        logger.info(f"âœ… {cleaned}ê°œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ\n")

    def process_pdf(self, pdf_path: Path) -> bool:
        """ë‹¨ì¼ PDF ì²˜ë¦¬"""
        try:
            logger.info(f"\n{'=' * 60}")
            logger.info(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_path.name}")
            logger.info(f"{'=' * 60}")
            
            # 1. PDF â†’ JSON
            logger.info("1ï¸âƒ£ PDF â†’ JSON ë³€í™˜")
            json_data = extract_pdf_to_json(str(pdf_path), str(self.output_dir))
            
            if not json_data:
                logger.error("JSON ë³€í™˜ ì‹¤íŒ¨")
                return False
            
            json_file = self.output_dir / f"{pdf_path.stem}.json"
            logger.info(f"   âœ… JSON ìƒì„±: {json_file.name}")
            
            return True
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False

    def normalize_all(self) -> bool:
        """ëª¨ë“  JSON ì •ê·œí™”"""
        logger.info("\n" + "=" * 60)
        logger.info("2ï¸âƒ£ ë°ì´í„° ì •ê·œí™”")
        logger.info("=" * 60)

        json_files = list(self.output_dir.glob("*.json"))
        json_files = [f for f in json_files if not f.name.startswith('batch_')]

        if not json_files:
            logger.error(f"âŒ {self.output_dir}ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False

        logger.info(f"ğŸ“‚ {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬")

        # JSON ë°ì´í„° ë¡œë“œ
        all_json_data = []
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                    all_json_data.append((json_file, json_data))
            except Exception as e:
                logger.error(f"JSON ë¡œë“œ ì‹¤íŒ¨ {json_file.name}: {e}")

        if not all_json_data:
            logger.error("ë¡œë“œëœ JSONì´ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # DB ì—°ê²° (PLAN_ID ë§¤ì¹­ìš©)
        db_manager = None
        if not self.skip_db and DB_AVAILABLE:
            try:
                db_manager = OracleDBManager(ORACLE_CONFIG)
                db_manager.connect()
                logger.info("ğŸ”— DB ì—°ê²° (PLAN_ID ë§¤ì¹­ìš©)")
            except Exception as e:
                logger.warning(f"âš ï¸ DB ì—°ê²° ì‹¤íŒ¨: {e}")
                db_manager = None

        # Normalizer ì´ˆê¸°í™”
        normalizer = GovernmentStandardNormalizer(
            str(json_files[0]),
            str(self.normalized_dir),
            db_manager=db_manager
        )

        # ê° JSON íŒŒì¼ ì²˜ë¦¬
        for json_file, json_data in all_json_data:
            logger.info(f"ğŸ“‹ ì •ê·œí™” ì¤‘: {json_file.name}")

            # íŒŒì¼ëª…ì—ì„œ ì—°ë„ ì¶”ì¶œ
            year_match = re.search(r'(20\d{2})', json_file.stem)
            if year_match:
                doc_year = int(year_match.group(1))
                normalizer.current_context['document_year'] = doc_year
                normalizer.current_context['performance_year'] = doc_year - 1
                normalizer.current_context['plan_year'] = doc_year
                logger.info(f"   ğŸ“… ì—°ë„: {doc_year}ë…„")

            normalizer.normalize(json_data)

        # CSV ì €ì¥
        normalizer.save_to_csv()

        # DB ì—°ê²° ì¢…ë£Œ
        if db_manager:
            db_manager.close()

        # í†µê³„ ì—…ë°ì´íŠ¸
        for table_name, records in normalizer.data.items():
            if isinstance(records, list):
                self.stats['total_records'] += len(records)

        normalizer.print_statistics()
        logger.info("âœ… ì •ê·œí™” ì™„ë£Œ")

        return True

    def load_to_database(self) -> bool:
        """Oracle DB ì ì¬"""
        if self.skip_db:
            logger.info("\nâ­ï¸ DB ì ì¬ ê±´ë„ˆëœ€")
            return True

        if not DB_AVAILABLE:
            logger.warning("âš ï¸ DB ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        try:
            logger.info("\n" + "=" * 80)
            logger.info("3ï¸âƒ£ Oracle ë°ì´í„°ë² ì´ìŠ¤ ì ì¬")
            logger.info("=" * 80)

            loader = OracleDirectLoader(
                db_config_read=ORACLE_CONFIG,
                db_config_write=ORACLE_CONFIG_DEV,
                csv_dir=str(self.normalized_dir)
            )
            loader.connect()

            logger.info("\nğŸ“‹ ì ì¬ íë¦„:")
            logger.info("   1ï¸âƒ£ BICS.TB_PLAN_DATA ì¡°íšŒ (ë§¤ì¹­ìš©)")
            logger.info("   2ï¸âƒ£ CSVì™€ ë§¤ì¹­ (YEAR + BIZ_NM + DETAIL_BIZ_NM)")
            logger.info("   3ï¸âƒ£ í•˜ìœ„ í…Œì´ë¸” ì ì¬ â†’ BICS_DEV")

            loader.load_with_matching()

            # í†µê³„
            stats = loader.load_stats
            logger.info(f"\nâœ… ì ì¬ ì™„ë£Œ: {stats['total_records']:,}ê±´")
            logger.info(f"   - ë§¤ì¹­ ì„±ê³µ: {stats['matched']}ê±´")
            logger.info(f"   - ë§¤ì¹­ ì‹¤íŒ¨: {stats['unmatched']}ê±´")

            loader.close()

            self.stats['db_loaded'] = True
            self.stats['matched'] = stats['matched']
            self.stats['unmatched'] = stats['unmatched']

            return True

        except Exception as e:
            logger.error(f"âŒ Oracle DB ì ì¬ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def run(self, pdf_files: List[str] = None):
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸš€ PDF to Database íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("=" * 80)
        
        # ì´ì „ ë°ì´í„° ì •ë¦¬
        self.clean_previous_data()

        # PDF íŒŒì¼ ì°¾ê¸°
        if pdf_files:
            pdf_list = [Path(f) for f in pdf_files if Path(f).exists()]
        else:
            pdf_list = list(self.input_dir.glob("*.pdf"))
        
        if not pdf_list:
            logger.error(f"âŒ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            logger.error(f"   '{self.input_dir}' í´ë”ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
            return False

        logger.info(f"ğŸ“‚ {len(pdf_list)}ê°œ PDF íŒŒì¼ ë°œê²¬\n")

        # 1ë‹¨ê³„: PDF â†’ JSON
        for pdf_path in pdf_list:
            self.stats['pdf_files'].append(pdf_path.name)
            if self.process_pdf(pdf_path):
                self.stats['processed'] += 1
            else:
                self.stats['failed'] += 1

        if self.stats['processed'] == 0:
            logger.error("âŒ ì²˜ë¦¬ëœ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # 2ë‹¨ê³„: JSON â†’ CSV ì •ê·œí™”
        if not self.normalize_all():
            logger.error("âŒ ì •ê·œí™” ì‹¤íŒ¨")
            return False

        # 3ë‹¨ê³„: DB ì ì¬
        if not self.skip_db:
            self.load_to_database()

        # ìµœì¢… í†µê³„
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
        logger.info("=" * 80)
        logger.info(f"PDF ì²˜ë¦¬: {self.stats['processed']}/{len(pdf_list)} ì„±ê³µ")
        logger.info(f"ì´ ë ˆì½”ë“œ: {self.stats['total_records']:,}ê±´")
        logger.info(f"DB ì ì¬: {'âœ…' if self.stats['db_loaded'] else 'â­ï¸ ê±´ë„ˆëœ€'}")
        logger.info(f"ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        logger.info("=" * 80 + "\n")

        return True


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ìƒëª…ê³µí•™ìœ¡ì„±ì‹œí–‰ê³„íš PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  python main.py                    # input í´ë”ì˜ ëª¨ë“  PDF ì²˜ë¦¬
  python main.py doc1.pdf doc2.pdf  # íŠ¹ì • PDF íŒŒì¼ ì²˜ë¦¬
  python main.py --skip-db          # DB ì ì¬ ê±´ë„ˆë›°ê¸° (CSVë§Œ ìƒì„±)
        """
    )
    
    parser.add_argument(
        'pdf_files',
        nargs='*',
        help='ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ (ìƒëµí•˜ë©´ input í´ë” ê²€ìƒ‰)'
    )
    
    parser.add_argument(
        '--skip-db',
        action='store_true',
        help='ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ê±´ë„ˆë›°ê¸° (CSVë§Œ ìƒì„±)'
    )
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = PDFtoDBPipeline(skip_db=args.skip_db)
    success = pipeline.run(args.pdf_files)
    
    if success:
        print("\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨")
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
