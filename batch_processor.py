#!/usr/bin/env python3
"""
ëŒ€ëŸ‰ PDF ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“ˆ
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
"""

import gc
import logging
from pathlib import Path
from typing import List, Dict, Any, Callable
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from datetime import datetime
import json

# í”„ë¡œê·¸ë ˆìŠ¤ ë°”
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    tqdm = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BatchPDFProcessor:
    """ëŒ€ëŸ‰ PDF ë°°ì¹˜ ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        input_dir: str = None,
        output_dir: str = None,
        batch_size: int = None,
        max_workers: int = None,
        use_multiprocessing: bool = None
    ):
        """
        Args:
            input_dir: ì…ë ¥ PDF ë””ë ‰í† ë¦¬ (Noneì´ë©´ config ì‚¬ìš©)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ config ì‚¬ìš©)
            batch_size: ë°°ì¹˜ë‹¹ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜ (Noneì´ë©´ config ì‚¬ìš©)
            max_workers: ë³‘ë ¬ ì‘ì—…ì ìˆ˜ (Noneì´ë©´ config ì‚¬ìš©)
            use_multiprocessing: True=í”„ë¡œì„¸ìŠ¤, False=ì“°ë ˆë“œ (Noneì´ë©´ config ì‚¬ìš©)
        """
        # configì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
        try:
            from config import INPUT_DIR, OUTPUT_DIR, BATCH_SIZE, MAX_WORKERS, USE_MULTIPROCESSING
            self.input_dir = Path(input_dir) if input_dir else INPUT_DIR
            self.output_dir = Path(output_dir) if output_dir else OUTPUT_DIR
            self.batch_size = batch_size if batch_size is not None else BATCH_SIZE
            self.max_workers = max_workers if max_workers is not None else MAX_WORKERS
            self.use_multiprocessing = use_multiprocessing if use_multiprocessing is not None else USE_MULTIPROCESSING
        except ImportError:
            # config ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            self.input_dir = Path(input_dir) if input_dir else Path("input")
            self.output_dir = Path(output_dir) if output_dir else Path("output")
            self.batch_size = batch_size if batch_size is not None else 10
            self.max_workers = max_workers if max_workers is not None else 4
            self.use_multiprocessing = use_multiprocessing if use_multiprocessing is not None else False

        # í†µê³„
        self.stats = {
            'total_files': 0,
            'processed': 0,
            'failed': 0,
            'skipped': 0,
            'start_time': None,  # type: Optional[datetime]
            'end_time': None,  # type: Optional[datetime]
            'errors': []
        }

    def get_pdf_files(self, recursive: bool = False) -> List[Path]:
        """PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        if recursive:
            pdf_files = list(self.input_dir.rglob("*.pdf"))
        else:
            pdf_files = list(self.input_dir.glob("*.pdf"))

        return sorted(pdf_files)

    def process_single_pdf(
        self,
        pdf_path: Path,
        processor_func: Callable,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ PDF ì²˜ë¦¬ (ë³„ë„ í”„ë¡œì„¸ìŠ¤/ì“°ë ˆë“œì—ì„œ ì‹¤í–‰)

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            processor_func: ì²˜ë¦¬ í•¨ìˆ˜
            **kwargs: ì²˜ë¦¬ í•¨ìˆ˜ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            'file': pdf_path.name,
            'path': str(pdf_path),
            'status': 'processing',
            'error': None,
            'data': None
        }

        try:
            # ì²˜ë¦¬ í•¨ìˆ˜ ì‹¤í–‰
            result['data'] = processor_func(pdf_path, **kwargs)
            result['status'] = 'success'

        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨ {pdf_path.name}: {e}")

        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()

        return result

    def process_batch(
        self,
        pdf_files: List[Path],
        processor_func: Callable,
        show_progress: bool = True,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ì²˜ë¦¬

        Args:
            pdf_files: ì²˜ë¦¬í•  PDF íŒŒì¼ ëª©ë¡
            processor_func: ê° PDFë¥¼ ì²˜ë¦¬í•  í•¨ìˆ˜
            show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
            **kwargs: processor_funcì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        total = len(pdf_files)

        # Executor ì„ íƒ
        ExecutorClass = ProcessPoolExecutor if self.use_multiprocessing else ThreadPoolExecutor

        # ì§„í–‰ë¥  í‘œì‹œ ì¤€ë¹„
        if show_progress and TQDM_AVAILABLE:
            progress = tqdm(total=total, desc="PDF ì²˜ë¦¬ ì¤‘")
        else:
            progress = None

        try:
            with ExecutorClass(max_workers=self.max_workers) as executor:
                # ì‘ì—… ì œì¶œ
                futures = {
                    executor.submit(
                        self.process_single_pdf,
                        pdf_path,
                        processor_func,
                        **kwargs
                    ): pdf_path
                    for pdf_path in pdf_files
                }

                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)

                    # í†µê³„ ì—…ë°ì´íŠ¸
                    if result['status'] == 'success':
                        self.stats['processed'] += 1
                    elif result['status'] == 'failed':
                        self.stats['failed'] += 1
                        self.stats['errors'].append({
                            'file': result['file'],
                            'error': result['error']
                        })

                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    if progress:
                        progress.update(1)
                        progress.set_postfix({
                            'success': self.stats['processed'],
                            'failed': self.stats['failed']
                        })

        finally:
            if progress:
                progress.close()

            # ë°°ì¹˜ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()

        return results

    def process_all(
        self,
        processor_func: Callable,
        recursive: bool = False,
        save_results: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬

        Args:
            processor_func: PDF ì²˜ë¦¬ í•¨ìˆ˜
            recursive: í•˜ìœ„ í´ë” í¬í•¨ ì—¬ë¶€
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            **kwargs: processor_funcì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            ì „ì²´ ì²˜ë¦¬ ê²°ê³¼
        """
        self.stats['start_time'] = datetime.now()

        # PDF íŒŒì¼ ì°¾ê¸°
        pdf_files = self.get_pdf_files(recursive)
        self.stats['total_files'] = len(pdf_files)

        if not pdf_files:
            logger.warning(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.input_dir}")
            return self.get_summary()

        logger.info(f"ì´ {len(pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}, ì‘ì—…ì: {self.max_workers}")

        all_results = []

        # ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬
        for i in range(0, len(pdf_files), self.batch_size):
            batch = pdf_files[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(pdf_files) + self.batch_size - 1) // self.batch_size

            logger.info(f"\në°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íŒŒì¼)")

            # ë°°ì¹˜ ì²˜ë¦¬
            batch_results = self.process_batch(
                batch,
                processor_func,
                show_progress=True,
                **kwargs
            )

            all_results.extend(batch_results)

            # ì¤‘ê°„ ì €ì¥ (ì„ íƒ)
            if save_results and batch_num % 5 == 0:
                self._save_intermediate_results(all_results, batch_num)

        self.stats['end_time'] = datetime.now()

        # ìµœì¢… ê²°ê³¼ ì €ì¥
        if save_results:
            self._save_final_results(all_results)

        return self.get_summary(all_results)

    def _save_intermediate_results(self, results: List[Dict], batch_num: int):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        try:
            output_file = self.output_dir / f"batch_results_{batch_num:04d}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {output_file}")
        except Exception as e:
            logger.error(f"ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _save_final_results(self, results: List[Dict]):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        try:
            # ì „ì²´ ê²°ê³¼
            output_file = self.output_dir / f"batch_results_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)

            # ìš”ì•½ ë¦¬í¬íŠ¸
            summary = self.get_summary(results)
            report_file = self.output_dir / f"batch_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2, default=str)

            logger.info(f"ìµœì¢… ê²°ê³¼ ì €ì¥: {output_file}")
            logger.info(f"ìš”ì•½ ë¦¬í¬íŠ¸: {report_file}")

        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_summary(self, results: List[Dict] = None) -> Dict[str, Any]:
        """ì²˜ë¦¬ ìš”ì•½ ì •ë³´"""
        duration = None
        if self.stats['start_time'] and self.stats['end_time']:
            duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()

        summary = {
            'total_files': self.stats['total_files'],
            'processed': self.stats['processed'],
            'failed': self.stats['failed'],
            'skipped': self.stats['skipped'],
            'success_rate': (
                f"{(self.stats['processed'] / self.stats['total_files'] * 100):.1f}%"
                if self.stats['total_files'] > 0 else "0%"
            ),
            'duration_seconds': duration,
            'start_time': self.stats['start_time'].isoformat() if self.stats['start_time'] else None,
            'end_time': self.stats['end_time'].isoformat() if self.stats['end_time'] else None,
            'errors': self.stats['errors'][:10],  # ìµœëŒ€ 10ê°œë§Œ
            'total_errors': len(self.stats['errors'])
        }

        if results:
            summary['results'] = results

        return summary

    def print_summary(self):
        """ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        summary = self.get_summary()

        print("\n" + "="*80)
        print("ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½")
        print("="*80)
        print(f"ì´ íŒŒì¼:     {summary['total_files']:,}ê°œ")
        print(f"ì²˜ë¦¬ ì„±ê³µ:   {summary['processed']:,}ê°œ")
        print(f"ì²˜ë¦¬ ì‹¤íŒ¨:   {summary['failed']:,}ê°œ")
        print(f"ì„±ê³µë¥ :      {summary['success_rate']}")

        if summary['duration_seconds']:
            print(f"ì†Œìš” ì‹œê°„:   {summary['duration_seconds']:.1f}ì´ˆ")
            if summary['processed'] > 0:
                avg_time = summary['duration_seconds'] / summary['processed']
                print(f"í‰ê·  ì²˜ë¦¬:   {avg_time:.2f}ì´ˆ/íŒŒì¼")

        if summary['total_errors'] > 0:
            print(f"\nâš ï¸ ì˜¤ë¥˜ ë°œìƒ: {summary['total_errors']}ê±´")
            print("ìµœê·¼ ì˜¤ë¥˜:")
            for error in summary['errors'][:5]:
                print(f"  - {error['file']}: {error['error'][:60]}...")

        print("="*80 + "\n")


# í—¬í¼ í•¨ìˆ˜: ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ê³¼ í†µí•©
def _pdf_processor_worker(pdf_path: Path, output_dir: str) -> Dict[str, Any]:
    """PDF â†’ JSON ë³€í™˜ (ìµœìƒìœ„ ë ˆë²¨ í•¨ìˆ˜ - ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    from extract_pdf_to_json import extract_pdf_to_json

    try:
        json_data = extract_pdf_to_json(str(pdf_path), output_dir)
        return {
            'status': 'success',
            'json_file': str(Path(output_dir) / f"{pdf_path.stem}.json"),
            'data': json_data
        }
    except Exception as e:
        raise Exception(f"PDF ë³€í™˜ ì‹¤íŒ¨: {e}")


def create_pdf_processor_func(output_dir: str):
    """PDF ì²˜ë¦¬ í•¨ìˆ˜ íŒ©í† ë¦¬"""
    from functools import partial
    return partial(_pdf_processor_worker, output_dir=output_dir)


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    import argparse

    parser = argparse.ArgumentParser(description="ëŒ€ëŸ‰ PDF ë°°ì¹˜ ì²˜ë¦¬")
    parser.add_argument('--input', default='input', help='ì…ë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', default='output', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--batch-size', type=int, default=10, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--workers', type=int, default=4, help='ë³‘ë ¬ ì‘ì—…ì ìˆ˜')
    parser.add_argument('--recursive', action='store_true', help='í•˜ìœ„ í´ë” í¬í•¨')

    args = parser.parse_args()

    # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ìƒì„±
    processor = BatchPDFProcessor(
        input_dir=args.input,
        output_dir=args.output,
        batch_size=args.batch_size,
        max_workers=args.workers
    )

    # ì²˜ë¦¬ í•¨ìˆ˜ ìƒì„±
    pdf_processor = create_pdf_processor_func(args.output)

    # ì‹¤í–‰
    summary = processor.process_all(
        pdf_processor,
        recursive=args.recursive
    )

    # ê²°ê³¼ ì¶œë ¥
    processor.print_summary()

